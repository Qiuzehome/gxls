import json
import asyncio
import time
import hashlib
import os
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
from concurrent.futures import ThreadPoolExecutor
import logging

# ===== é»˜è®¤é…ç½®å‚æ•° =====

# ç¼“å­˜é…ç½®
DEFAULT_CACHE_EXPIRE_HOURS = 24  # ç¼“å­˜24å°æ—¶
DEFAULT_MAX_CACHE_SIZE = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
DEFAULT_CACHE_CLEANUP_SIZE = 500  # ç¼“å­˜æ¸…ç†ä¿ç•™æ•°é‡

# é¡µé¢æ£€æŸ¥é…ç½®
DEFAULT_PAGE_LOAD_TIMEOUT = 5000  # DOMåŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
DEFAULT_NAVIGATION_TIMEOUT = 8000  # é¡µé¢å¯¼èˆªè¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰
DEFAULT_SECONDARY_PAGE_TIMEOUT = 6000  # äºŒçº§é¡µé¢æ£€æŸ¥è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰
DEFAULT_MAX_SECONDARY_LINKS = 2  # æœ€å¤šæ£€æŸ¥çš„äºŒçº§é“¾æ¥æ•°

# å¹¶è¡Œå¤„ç†é…ç½®
DEFAULT_MAX_CONCURRENT = 3  # é»˜è®¤å¹¶å‘ä¸Šä¸‹æ–‡æ•°
DEFAULT_PAGES_PER_CONTEXT = 4  # é»˜è®¤æ¯ä¸Šä¸‹æ–‡é¡µé¢æ•°

# ç®€å•çš„å†…å­˜ç¼“å­˜
_form_cache = {}
CACHE_EXPIRE_HOURS = DEFAULT_CACHE_EXPIRE_HOURS


async def check_forms_on_page(page, url, level=1):
    """æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æœ‰æ•ˆè¡¨å•ï¼ˆåŒ…å«inputå…ƒç´ çš„è¡¨å•ï¼‰"""
    try:
        # ä¼˜åŒ–ï¼šåªç­‰å¾…domcontentloadedï¼Œä¸ç­‰å¾…æ‰€æœ‰ç½‘ç»œè¯·æ±‚å®Œæˆ
        await page.wait_for_load_state(
            "domcontentloaded", timeout=DEFAULT_PAGE_LOAD_TIMEOUT
        )

        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„CSSé€‰æ‹©å™¨ç›´æ¥æŸ¥æ‰¾åŒ…å«inputçš„è¡¨å•
        valid_forms = await page.evaluate(
            """
            () => {
                const forms = document.querySelectorAll('form');
                let validCount = 0;
                for (const form of forms) {
                    if (form.querySelector('input')) {
                        validCount++;
                    }
                }
                return validCount;
            }
        """
        )

        if valid_forms > 0:
            print(
                f"{'  ' * (level-1)}âœ“ {level}çº§é¡µé¢ {url} åŒ…å« {valid_forms} ä¸ªæœ‰æ•ˆè¡¨å•ï¼ˆå«inputï¼‰"
            )
            return True, valid_forms

        # å¦‚æœä¸»é¡µé¢æ²¡æœ‰æœ‰æ•ˆè¡¨å•ï¼Œæ£€æŸ¥åŒæºiframe
        if valid_forms == 0:
            iframe_forms = await check_iframes_for_forms(page, url, level)
            if iframe_forms > 0:
                print(
                    f"{'  ' * (level-1)}âœ“ {level}çº§é¡µé¢ {url} åœ¨iframeä¸­æ‰¾åˆ° {iframe_forms} ä¸ªæœ‰æ•ˆè¡¨å•"
                )
                return True, iframe_forms

        print(f"{'  ' * (level-1)}âœ— {level}çº§é¡µé¢ {url} ä¸åŒ…å«æœ‰æ•ˆè¡¨å•")
        return False, 0

    except Exception as e:
        print(f"{'  ' * (level-1)}âœ— {level}çº§é¡µé¢ {url} æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False, 0


async def check_iframes_for_forms(page, url, level=1):
    """æ£€æŸ¥é¡µé¢ä¸­çš„åŒæºiframeæ˜¯å¦åŒ…å«æœ‰æ•ˆè¡¨å•"""
    try:
        # è·å–é¡µé¢çš„åŸŸå
        from urllib.parse import urlparse

        main_domain = urlparse(url).netloc

        # æŸ¥æ‰¾æ‰€æœ‰iframe
        iframes = await page.query_selector_all("iframe")

        if not iframes:
            return 0

        print(f"{'  ' * level}ğŸ” æ‰¾åˆ° {len(iframes)} ä¸ªiframeï¼Œæ£€æŸ¥åŒæºiframe...")

        total_iframe_forms = 0

        for i, iframe in enumerate(iframes):
            try:
                # è·å–iframeçš„srcå’Œsrcdocå±æ€§
                iframe_src = await iframe.get_attribute("src")
                iframe_srcdoc = await iframe.get_attribute("srcdoc")

                is_same_origin = False
                iframe_description = ""

                if not iframe_src and not iframe_srcdoc:
                    # æ²¡æœ‰srcå’Œsrcdocçš„iframeé€šå¸¸æ˜¯åŒæºçš„ï¼ˆåŠ¨æ€åˆ›å»ºæˆ–ç©ºiframeï¼‰
                    is_same_origin = True
                    iframe_description = "åŠ¨æ€/ç©ºiframe"
                elif iframe_srcdoc:
                    # ä½¿ç”¨srcdocçš„iframeæ˜¯åŒæºçš„
                    is_same_origin = True
                    iframe_description = "srcdocå†…è”iframe"
                elif iframe_src:
                    # æ£€æŸ¥srcæ˜¯å¦ä¸ºåŒæº
                    if iframe_src.startswith("//"):
                        iframe_src = f"https:{iframe_src}"
                    elif iframe_src.startswith("/"):
                        iframe_src = f"https://{main_domain}{iframe_src}"
                    elif not iframe_src.startswith(("http://", "https://")):
                        # ç›¸å¯¹è·¯å¾„ä¹Ÿæ˜¯åŒæºçš„
                        is_same_origin = True
                        iframe_description = f"ç›¸å¯¹è·¯å¾„iframe: {iframe_src}"
                    else:
                        iframe_domain = urlparse(iframe_src).netloc
                        if iframe_domain == main_domain:
                            is_same_origin = True
                            iframe_description = f"åŒæºiframe: {iframe_src}"
                        else:
                            print(
                                f"{'  ' * (level+1)}â­ iframe {i+1} è·¨åŸŸï¼Œè·³è¿‡: {iframe_domain}"
                            )
                            continue

                if not is_same_origin:
                    continue

                print(f"{'  ' * (level+1)}ğŸ” æ£€æŸ¥iframe {i+1}: {iframe_description}")

                # è·å–iframeå†…å®¹
                iframe_content = await iframe.content_frame()
                if iframe_content:
                    # åœ¨iframeä¸­æŸ¥æ‰¾è¡¨å•
                    iframe_forms = await iframe_content.query_selector_all("form")
                    iframe_valid_forms = 0

                    for form in iframe_forms:
                        inputs = await form.query_selector_all("input")
                        if inputs:
                            iframe_valid_forms += 1

                    if iframe_valid_forms > 0:
                        print(
                            f"{'  ' * (level+1)}âœ“ iframe {i+1} åŒ…å« {iframe_valid_forms} ä¸ªæœ‰æ•ˆè¡¨å•"
                        )
                        total_iframe_forms += iframe_valid_forms
                    else:
                        print(f"{'  ' * (level+1)}âœ— iframe {i+1} æ— æœ‰æ•ˆè¡¨å•")

            except Exception as e:
                print(f"{'  ' * (level+1)}âœ— iframe {i+1} æ£€æŸ¥å¤±è´¥: {str(e)}")
                continue

        return total_iframe_forms

    except Exception as e:
        print(f"{'  ' * level}âœ— iframeæ£€æŸ¥å¤±è´¥: {str(e)}")
        return 0


async def get_links_from_page(page, max_links=10):
    """è·å–é¡µé¢ä¸­çš„é“¾æ¥ï¼Œé™åˆ¶æ•°é‡ä»¥æé«˜æ•ˆç‡ï¼Œä¼˜å…ˆè¿”å›åŒ…å«contactçš„é“¾æ¥"""
    try:
        # æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾çš„hrefå±æ€§
        links = await page.evaluate(
            """
            () => {
                const links = Array.from(document.querySelectorAll('a[href]'));
                return links
                    .map(link => link.href)
                    .filter(href => {
                        // è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
                        try {
                            const url = new URL(href);
                            return url.protocol === 'http:' || url.protocol === 'https:';
                        } catch {
                            return false;
                        }
                    });
            }
        """
        )

        # å°†åŒ…å« /contact çš„é“¾æ¥ä¼˜å…ˆæ’åº
        contact_links = [link for link in links if "/contact" in link.lower()]
        other_links = [link for link in links if "/contact" not in link.lower()]

        # åˆå¹¶å¹¶é™åˆ¶æ•°é‡
        prioritized_links = contact_links + other_links
        return prioritized_links[:max_links]

    except Exception as e:
        print(f"è·å–é“¾æ¥å¤±è´¥: {str(e)}")
        return []


def normalize_url(url):
    """æ ‡å‡†åŒ–URLï¼Œæ·»åŠ åè®®å‰ç¼€"""
    if not url.startswith(("http://", "https://")):
        return f"https://{url}"
    return url


def get_cache_key(url):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(url.encode()).hexdigest()


def is_cache_valid(timestamp):
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
    if not timestamp:
        return False
    cache_time = datetime.fromisoformat(timestamp)
    return datetime.now() - cache_time < timedelta(hours=CACHE_EXPIRE_HOURS)


def get_cached_result(url):
    """è·å–ç¼“å­˜çš„ç»“æœ"""
    cache_key = get_cache_key(url)
    if cache_key in _form_cache:
        cached_data = _form_cache[cache_key]
        if is_cache_valid(cached_data.get("timestamp")):
            print(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜ç»“æœ: {url}")
            return cached_data.get("has_forms")
    return None


def set_cached_result(url, has_forms):
    """è®¾ç½®ç¼“å­˜ç»“æœ"""
    cache_key = get_cache_key(url)
    _form_cache[cache_key] = {
        "has_forms": has_forms,
        "timestamp": datetime.now().isoformat(),
    }
    # ç®€å•çš„ç¼“å­˜æ¸…ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤šï¼Œæ¸…ç†æ—§çš„
    if len(_form_cache) > DEFAULT_MAX_CACHE_SIZE:
        # æ¸…ç†ä¸€åŠçš„æ—§ç¼“å­˜
        old_keys = list(_form_cache.keys())[:DEFAULT_CACHE_CLEANUP_SIZE]
        for key in old_keys:
            del _form_cache[key]


async def check_url_with_forms(page, url):
    """æ£€æŸ¥URLåŠå…¶äºŒçº§é¡µé¢æ˜¯å¦åŒ…å«è¡¨å•ï¼ˆä¼˜åŒ–+ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    normalized_url = normalize_url(url)

    # æ£€æŸ¥ç¼“å­˜
    cached_result = get_cached_result(normalized_url)
    if cached_result is not None:
        return cached_result

    print(f"ğŸ” æ£€æŸ¥: {normalized_url}")

    result = False
    try:
        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„å¯¼èˆªç­–ç•¥
        await page.goto(
            normalized_url,
            timeout=DEFAULT_NAVIGATION_TIMEOUT,
            wait_until="domcontentloaded",
        )

        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å¿«çš„è¡¨å•æ£€æµ‹
        has_form, form_count = await check_forms_on_page(page, normalized_url, 1)

        if has_form:
            result = True
        else:
            # ä¼˜åŒ–ï¼šåªæ£€æŸ¥æœ€ç›¸å…³çš„äºŒçº§é¡µé¢ï¼ˆcontactç›¸å…³é“¾æ¥ï¼‰
            try:
                # ä¼˜åŒ–ï¼šåŒæ—¶è·å–é“¾æ¥å’Œæ‰§è¡Œå¿«é€Ÿè¡¨å•æ£€æµ‹
                contact_links = await page.evaluate(
                    f"""
                    () => {{
                        const links = Array.from(document.querySelectorAll('a[href*="contact"], a[href*="form"], a[href*="inquiry"]'));
                        return links.slice(0, {DEFAULT_MAX_SECONDARY_LINKS}).map(link => link.href).filter(href => {{
                            try {{
                                const url = new URL(href);
                                return url.protocol === 'http:' || url.protocol === 'https:';
                            }} catch {{
                                return false;
                            }}
                        }});
                    }}
                """
                )

                if contact_links:
                    print(f"ğŸ“‹ æ£€æŸ¥ {len(contact_links)} ä¸ªç›¸å…³é“¾æ¥...")

                    # å¿«é€Ÿæ£€æŸ¥contactç›¸å…³é¡µé¢
                    for link in contact_links:
                        try:
                            await page.goto(
                                link,
                                timeout=DEFAULT_SECONDARY_PAGE_TIMEOUT,
                                wait_until="domcontentloaded",
                            )
                            has_form, _ = await check_forms_on_page(page, link, 2)

                            if has_form:
                                print(f"âœ… åœ¨ç›¸å…³é¡µé¢æ‰¾åˆ°è¡¨å•ï¼{link}")
                                result = True
                                break

                        except Exception as e:
                            print(f"âŒ ç›¸å…³é¡µé¢æ£€æŸ¥å¤±è´¥: {str(e)}")
                            continue

            except Exception as e:
                print(f"âŒ é“¾æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")

    except Exception as e:
        print(f"âŒ æ— æ³•è®¿é—®: {str(e)}")
        result = False

    # ç¼“å­˜ç»“æœ
    set_cached_result(normalized_url, result)
    return result


async def check_single_url_with_page(page, url, page_id):
    """ä½¿ç”¨å•ä¸ªé¡µé¢æ£€æŸ¥å•ä¸ªURL"""
    try:
        has_forms = await check_url_with_forms(page, url)
        if has_forms:
            print(f"âœ… é¡µé¢{page_id}: {url} åŒ…å«è¡¨å•")
            return url
        else:
            print(f"âŒ é¡µé¢{page_id}: {url} æ— è¡¨å•")
            return None
    except Exception as e:
        print(f"âŒ é¡µé¢{page_id}: {url} æ£€æŸ¥å¤±è´¥: {str(e)}")
        return None


async def check_url_batch_multi_page(
    browser, urls_batch, batch_id, pages_per_context=4
):
    """æ‰¹é‡æ£€æŸ¥URLï¼ˆå¤šé¡µé¢å¹¶è¡Œå¤„ç†ï¼‰"""
    context = await browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        viewport={"width": 1280, "height": 720},
        bypass_csp=True,
    )

    # ç¦ç”¨å›¾ç‰‡ã€CSSã€å­—ä½“ç­‰éå¿…è¦èµ„æº
    await context.route(
        "**/*",
        lambda route: (
            route.abort()
            if route.request.resource_type in ["image", "stylesheet", "font", "media"]
            else route.continue_()
        ),
    )

    # åˆ›å»ºå¤šä¸ªé¡µé¢
    pages = []
    for i in range(pages_per_context):
        page = await context.new_page()
        page.set_default_timeout(8000)
        pages.append(page)

    print(
        f"ğŸ“¦ æ‰¹æ¬¡ {batch_id}: ä½¿ç”¨ {len(pages)} ä¸ªé¡µé¢å¹¶è¡Œæ£€æŸ¥ {len(urls_batch)} ä¸ªURL"
    )

    results = []

    # å°†URLåˆ†é…ç»™ä¸åŒé¡µé¢å¹¶è¡Œå¤„ç†
    tasks = []
    for i, url in enumerate(urls_batch):
        page_index = i % len(pages)
        page = pages[page_index]
        page_id = f"{batch_id}-P{page_index+1}"

        task = check_single_url_with_page(page, url, page_id)
        tasks.append(task)

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    task_results = await asyncio.gather(*tasks, return_exceptions=True)

    # æ”¶é›†æœ‰æ•ˆç»“æœ
    for result in task_results:
        if isinstance(result, str):  # æˆåŠŸè¿”å›URL
            results.append(result)
        elif isinstance(result, Exception):
            print(f"âš ï¸ æ‰¹æ¬¡ {batch_id}: ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")

    await context.close()
    print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_id}: å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
    return results


# ä¿ç•™åŸæ¥çš„å•é¡µé¢ç‰ˆæœ¬ä½œä¸ºå¤‡é€‰
async def check_url_batch(browser, urls_batch, batch_id):
    """æ‰¹é‡æ£€æŸ¥URLï¼ˆå•é¡µé¢å¤„ç†ï¼Œå¤‡é€‰æ–¹æ¡ˆï¼‰"""
    return await check_url_batch_multi_page(
        browser, urls_batch, batch_id, pages_per_context=1
    )


async def load_url(
    urls,
    max_concurrent=DEFAULT_MAX_CONCURRENT,
    pages_per_context=DEFAULT_PAGES_PER_CONTEXT,
):
    """ä¸»å‡½æ•°ï¼šå¤šé¡µé¢å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰URLæ˜¯å¦åŒ…å«è¡¨å•"""
    async with async_playwright() as p:
        # ä¼˜åŒ–æµè§ˆå™¨å¯åŠ¨å‚æ•°
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-gpu",
                "--disable-background-timer-throttling",
                "--disable-backgrounding-occluded-windows",
                "--disable-renderer-backgrounding",
                "--disable-features=TranslateUI",
                "--disable-ipc-flooding-protection",
                "--memory-pressure-off",
                "--disable-web-security",  # å…è®¸è·¨åŸŸï¼Œæé«˜å…¼å®¹æ€§
            ],
        )

        total_urls = len(urls)
        total_pages = max_concurrent * pages_per_context
        print(f"ğŸš€ å¼€å§‹å¤šé¡µé¢å¹¶è¡Œæ£€æŸ¥ {total_urls} ä¸ªURL")
        print(
            f"ğŸ“Š é…ç½®: {max_concurrent} ä¸ªä¸Šä¸‹æ–‡ Ã— {pages_per_context} ä¸ªé¡µé¢ = {total_pages} ä¸ªå¹¶è¡Œé¡µé¢"
        )

        # å°†URLåˆ†æ‰¹å¤„ç†ï¼Œè€ƒè™‘æ¯ä¸ªä¸Šä¸‹æ–‡å†…çš„é¡µé¢æ•°
        batch_size = max(1, total_urls // max_concurrent)
        url_batches = [
            urls[i : i + batch_size] for i in range(0, total_urls, batch_size)
        ]

        print(f"ğŸ“¦ åˆ†ä¸º {len(url_batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹çº¦ {batch_size} ä¸ªURL")

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡å†…éƒ¨ä½¿ç”¨å¤šé¡µé¢å¹¶è¡Œ
        tasks = []
        for batch_id, batch in enumerate(url_batches, 1):
            task = check_url_batch_multi_page(
                browser, batch, batch_id, pages_per_context
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)

        # åˆå¹¶ç»“æœ
        all_results = []
        for batch_result in batch_results:
            if isinstance(batch_result, list):
                all_results.extend(batch_result)
            else:
                print(f"âš ï¸ æ‰¹æ¬¡æ‰§è¡Œå‡ºé”™: {batch_result}")

        await browser.close()

        print(f"ğŸ¯ å¤šé¡µé¢å¹¶è¡Œæ£€æŸ¥å®Œæˆï¼æ€»è®¡æ‰¾åˆ° {len(all_results)} ä¸ªæœ‰æ•ˆç»“æœ")
        print(f"ğŸ“ˆ å®é™…å¹¶è¡Œåº¦: {total_pages} ä¸ªé¡µé¢åŒæ—¶å·¥ä½œ")
        return all_results


async def load_url_single_page(urls, max_concurrent=DEFAULT_MAX_CONCURRENT):
    """å•é¡µé¢ç‰ˆæœ¬ï¼ˆå…¼å®¹æ€§å¤‡é€‰æ–¹æ¡ˆï¼‰"""
    return await load_url(urls, max_concurrent, pages_per_context=1)


# è¿è¡Œæ£€æŸ¥
if __name__ == "__main__":
    asyncio.run(load_url())
